{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24b79df6-e835-44be-9f5f-704404108dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Combining Data ---\n",
      "Successfully loaded data for year 2018.\n",
      "Successfully loaded data for year 2019.\n",
      "Successfully loaded data for year 2020.\n",
      "Successfully loaded data for year 2021.\n",
      "Successfully loaded data for year 2022.\n",
      "Combined data from 5 years into a master DataFrame (initial load).\n",
      "--- Step 2: Cleaning Column Names ---\n",
      "Stripped whitespace from column names.\n",
      "--- Step 3: Cleaning Data Values ---\n",
      "Removed leading/trailing characters (','. ') from string columns.\n",
      "--- Step 4: Dropping Specified Columns ---\n",
      "Dropped columns: ['Hospital Total Days Title V For Adults & Peds', 'Hospital Total Days Title XVIII For Adults & Peds', 'Hospital Total Days Title XIX For Adults & Peds']\n",
      "--- Step 5: Dropping Rows with Missing CBSA ---\n",
      "Dropped 382 rows with missing or empty 'Medicare CBSA Number'. Remaining rows: 30075\n",
      "--- Step 6: Creating Derived Financial Columns (Staffing FTE, Total Revenue) ---\n",
      "  Combining staffing FTE columns: ['FTE - Employees on Payroll', 'Number of Interns and Residents (FTE)'] into 'Total_Staffing_FTE'\n",
      "  Created 'Total_Staffing_FTE' column.\n",
      "  Dropped original staffing FTE columns: ['FTE - Employees on Payroll', 'Number of Interns and Residents (FTE)']\n",
      "  Creating 'Total Revenue' column.\n",
      "  Created 'Total Revenue' column.\n",
      "--- Step 7: Creating Facility ID ---\n",
      "Created 'facility_id' column.\n",
      "--- Step 8: Filtering for Facilities in All 5 Years ---\n",
      "Filtered for facilities reporting in all 5 file years. Remaining unique facilities: 4924\n",
      "\n",
      "--- Step 9: Renaming Columns (Initial and Specific) ---\n",
      "--- Step 9: Renaming Columns ---\n",
      "Renamed specified columns: ['Less: Allowances for Uncollectible Notes and Accounts Receivable', \"Less Contractual Allowance and Discounts on Patients' Accounts\", 'Hospital Total Days (V + XVIII + XIX + Unknown) For Adults & Peds']\n",
      "--- Step 10: Loading and Processing Inflation Data from: C:\\Users\\yadla\\OneDrive\\Desktop\\Hospital_Analysis_Project\\Inflation Data\\file.csv ---\n",
      "Successfully loaded inflation data.\n",
      "Inflation data processed (selected 'Year' and 'Annual', converted types).\n",
      "--- Step 11: Loading and Processing Medicare Enrollment Data from: C:/Users/yadla/OneDrive/Desktop/Hospital_Analysis_Project/Medicare Data/Medicare_Enrollment_Processed.csv ---\n",
      "Successfully loaded processed Medicare enrollment data.\n",
      "Aggregating Medicare data to yearly total beneficiaries per state...\n",
      "Medicare data aggregated and columns renamed.\n",
      "--- Step 12: Merging Inflation Data ---\n",
      "Inflation data merged.\n",
      "--- Step 13: Merging Medicare Enrollment Data ---\n",
      "Medicare Enrollment data merged.\n",
      "--- Data Integration Complete ---\n",
      "\n",
      "--- Step 14: Selecting Final Financial Subset Columns ---\n",
      "Selected 55 columns for the final subset.\n",
      "\n",
      "--- Step 15: Starting Missing Value Imputation ---\n",
      "Initial total missing values in numeric columns to impute: 90712\n",
      "Final total missing values in numeric columns after imputation: 0\n",
      "--- Missing Value Imputation Complete ---\n",
      "Added 'Annual' to categorization map.\n",
      "Added 'Total_Yearly_Medicare_Enrollment' to categorization map.\n",
      "\n",
      "--- Using the Categorization Dictionary with Final Data ---\n",
      "Columns in the 'External Factors' category within the final subset: ['Annual', 'Total_Yearly_Medicare_Enrollment']\n",
      "\n",
      "Columns in Final Subset by Category (using updated map):\n",
      "- Balance Sheet - Assets: ['Other Assets', 'Prepaid Expenses', 'Total Other Assets', 'Buildings', 'Inventory', 'Major Movable Equipment', 'Cash on Hand and in Banks', 'Accounts Receivable', 'Total Fixed Assets', 'Total Current Assets', 'Total Assets']\n",
      "- Balance Sheet - Fund Balance/Equity: ['General Fund Balance', 'Total Fund Balances', 'Total Liabilities and Fund Balances']\n",
      "- Balance Sheet - Liabilities: ['Other Current Liabilities', 'Total Long Term Liabilities', 'Salaries, Wages, and Fees Payable', 'Accounts Payable', 'Total Current Liabilities', 'Total Liabilities']\n",
      "- Charges: ['Outpatient Total Charges', 'Inpatient Total Charges', 'Combined Outpatient + Inpatient Total Charges']\n",
      "- Expenses & Costs: ['Cost of Charity Care', 'Total Bad Debt Expense', 'Cost of Uncompensated Care', 'Total Unreimbursed and Uncompensated Care', \"Contractual Allowance and Discounts on Patients' Accounts\", 'Depreciation Cost', 'Less Total Operating Expense', 'Total Salaries From Worksheet A', 'Overhead Non-Salary Costs', 'Total Costs']\n",
      "- External Factors: ['Annual', 'Total_Yearly_Medicare_Enrollment']\n",
      "- Geography: ['State Code', 'County', 'Rural Versus Urban']\n",
      "- Identifier: ['facility_id', 'Hospital Name']\n",
      "- Key Ratios: ['Cost To Charge Ratio']\n",
      "- Operations: ['Total_Staffing_FTE', 'Number of Beds', 'Hospital Total Days Or Visits']\n",
      "- Payer & Reimbursement Specific: ['Net Revenue from Medicaid']\n",
      "- Profitability: ['Net Income', 'Net Income from Service to Patients']\n",
      "- Revenue: ['Total Revenue', 'Outpatient Revenue', 'Total Other Income', 'Inpatient Revenue', 'Total Patient Revenue', 'Net Patient Revenue', 'Total Income']\n",
      "- Time: ['Year']\n",
      "\n",
      "✅ Final Processed and Integrated DataFrame saved to: C:\\Users\\yadla\\OneDrive\\Desktop\\Hospital_Analysis_Project\\Data\\Hospital_Financial_Analysis_Subset_Integrated_Imputed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import timedelta # Although fiscal date processing is currently skipped\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define base data path and years to process\n",
    "DATA_PATH = r\"C:\\Users\\yadla\\OneDrive\\Desktop\\Hospital_Analysis_Project\\Data\"\n",
    "YEARS = range(2018, 2023) # 2018–2022 inclusive\n",
    "\n",
    "# Paths for external data files (Inflation and Processed Medicare Data)\n",
    "INFLATION_DATA_FILE = r\"C:\\Users\\yadla\\OneDrive\\Desktop\\Hospital_Analysis_Project\\Inflation Data\\file.csv\" # Path to your simple yearly inflation file\n",
    "PROCESSED_MEDICARE_FILE = r\"C:/Users/yadla/OneDrive/Desktop/Hospital_Analysis_Project/Medicare Data/Medicare_Enrollment_Processed.csv\" # Path to your processed Medicare file\n",
    "\n",
    "# Define the final output path for the integrated, imputed subset file\n",
    "FINAL_OUTPUT_FILE_PATH = os.path.join(DATA_PATH, \"Hospital_Financial_Analysis_Subset_Integrated_Imputed.csv\")\n",
    "\n",
    "# Columns to drop in the initial cleaning phase\n",
    "INITIAL_COLS_TO_DROP = [\n",
    "    \"Hospital Total Days Title V For Adults & Peds\",\n",
    "    \"Hospital Total Days Title XIX For Adults & Peds\",\n",
    "    \"Hospital Total Days Title XVIII For Adults & Peds\",\n",
    "]\n",
    "\n",
    "# Rename map for columns\n",
    "RENAME_MAP = {\n",
    "    \"Less: Allowances for Uncollectible Notes and Accounts Receivable\": (\n",
    "        \"Allowances for Uncollectible Notes and Accounts Receivable\"\n",
    "    ),\n",
    "    \"Less Contractual Allowance and Discounts on Patients' Accounts\": (\n",
    "        \"Contractual Allowance and Discounts on Patients' Accounts\"\n",
    "    ),\n",
    "    # Added renaming for the total hospital days/visits column\n",
    "    \"Hospital Total Days (V + XVIII + XIX + Unknown) For Adults & Peds\": \"Hospital Total Days Or Visits\",\n",
    "}\n",
    "\n",
    "# Define the exact list of hospital financial columns to select for the final subset\n",
    "# 'Rural Versus Urban' is INCLUDED in this list so it's in the output file.\n",
    "COLUMNS_FOR_FINAL_SUBSET = [\n",
    "    \"Year\",\n",
    "    \"facility_id\",\n",
    "    \"Total Revenue\",\n",
    "    \"Hospital Name\",\n",
    "    \"State Code\",\n",
    "    \"County\",\n",
    "    \"Rural Versus Urban\", # Included for visualization/EDA\n",
    "    \"Total_Staffing_FTE\", # Combined staffing column\n",
    "    \"Number of Beds\",\n",
    "    \"Cost of Charity Care\",\n",
    "    \"Net Revenue from Medicaid\",\n",
    "    \"Total Bad Debt Expense\",\n",
    "    \"Other Assets\",\n",
    "    \"Cost of Uncompensated Care\",\n",
    "    \"Total Unreimbursed and Uncompensated Care\",\n",
    "    \"Cost To Charge Ratio\",\n",
    "    \"Prepaid Expenses\",\n",
    "    \"Other Current Liabilities\",\n",
    "    \"Total Other Assets\",\n",
    "    \"Total Long Term Liabilities\",\n",
    "    \"Buildings\",\n",
    "    \"Salaries, Wages, and Fees Payable\",\n",
    "    \"Inventory\",\n",
    "    \"Major Movable Equipment\",\n",
    "    \"Outpatient Revenue\",\n",
    "    \"Cash on Hand and in Banks\",\n",
    "    \"Outpatient Total Charges\",\n",
    "    \"Accounts Payable\",\n",
    "    \"Accounts Receivable\",\n",
    "    \"Total Fixed Assets\",\n",
    "    \"Contractual Allowance and Discounts on Patients' Accounts\",\n",
    "    \"Total Current Liabilities\",\n",
    "    \"Total Liabilities\",\n",
    "    \"Total Current Assets\",\n",
    "    \"General Fund Balance\",\n",
    "    \"Total Fund Balances\",\n",
    "    \"Total Liabilities and Fund Balances\",\n",
    "    \"Total Assets\",\n",
    "    \"Total Other Income\",\n",
    "    \"Inpatient Revenue\",\n",
    "    \"Total Patient Revenue\",\n",
    "    \"Net Patient Revenue\",\n",
    "    \"Depreciation Cost\",\n",
    "    \"Total Income\",\n",
    "    \"Net Income\",\n",
    "    \"Less Total Operating Expense\",\n",
    "    \"Net Income from Service to Patients\",\n",
    "    \"Inpatient Total Charges\",\n",
    "    \"Total Salaries From Worksheet A\",\n",
    "    \"Overhead Non-Salary Costs\",\n",
    "    \"Total Costs\",\n",
    "    \"Combined Outpatient + Inpatient Total Charges\",\n",
    "    # Now requesting the RENAMED column name for the final subset selection\n",
    "    \"Hospital Total Days Or Visits\",\n",
    "    # Note: Columns from external data ('Annual', 'Total_Yearly_Medicare_Enrollment')\n",
    "    # will be added to the DataFrame during processing and are not listed here\n",
    "    # as they are not part of the *initial* selection list.\n",
    "]\n",
    "\n",
    "# Define the base categorization dictionary for the initial financial columns\n",
    "# 'Rural Versus Urban' is INCLUDED in this map.\n",
    "column_category_map = {\n",
    "    \"Year\": {'category': 'Time', 'subcategory': None},\n",
    "    \"facility_id\": {'category': 'Identifier', 'subcategory': None},\n",
    "    \"Hospital Name\": {'category': 'Identifier', 'subcategory': None},\n",
    "    \"State Code\": {'category': 'Geography', 'subcategory': 'State'},\n",
    "    \"County\": {'category': 'Geography', 'subcategory': 'County'},\n",
    "    \"Rural Versus Urban\": {'category': 'Geography', 'subcategory': 'Rural/Urban'}, # Included for categorization\n",
    "    \"Total_Staffing_FTE\": {'category': 'Operations', 'subcategory': 'Staffing'},\n",
    "    \"Number of Beds\": {'category': 'Operations', 'subcategory': 'Capacity'},\n",
    "    # Updated category map entry for the renamed column\n",
    "    \"Hospital Total Days Or Visits\":{'category': 'Operations', 'subcategory': 'Capacity'},\n",
    "\n",
    "    \"Inpatient Revenue\": {'category': 'Revenue', 'subcategory': 'Gross Patient Revenue'},\n",
    "    \"Outpatient Revenue\": {'category': 'Revenue', 'subcategory': 'Gross Patient Revenue'},\n",
    "    \"Total Patient Revenue\": {'category': 'Revenue', 'subcategory': 'Gross Patient Revenue'},\n",
    "    \"Net Patient Revenue\": {'category': 'Revenue', 'subcategory': 'Net Patient Revenue'},\n",
    "    \"Total Other Income\": {'category': 'Revenue', 'subcategory': 'Other Revenue'},\n",
    "    \"Total Income\": {'category': 'Revenue', 'subcategory': 'Total Income'},\n",
    "    \"Total Revenue\": {'category': 'Revenue', 'subcategory': 'Total Revenue'},\n",
    "\n",
    "    \"Less Total Operating Expense\": {'category': 'Expenses & Costs', 'subcategory': 'Operating Expenses (Total)'},\n",
    "    \"Contractual Allowance and Discounts on Patients' Accounts\": {'category': 'Expenses & Costs', 'subcategory': 'Costs'},\n",
    "    \"Salaries, Wages, and Fees Payable\": {'category': 'Expenses & Costs', 'subcategory': 'Costs'}, # Also a Liability\n",
    "    \"Total Salaries From Worksheet A\": {'category': 'Expenses & Costs', 'subcategory': 'Costs'},\n",
    "    \"Overhead Non-Salary Costs\": {'category': 'Expenses & Costs', 'subcategory': 'Costs'},\n",
    "    \"Depreciation Cost\": {'category': 'Expenses & Costs', 'subcategory': 'Costs'},\n",
    "    \"Total Costs\": {'category': 'Expenses & Costs', 'subcategory': 'Costs'},\n",
    "    \"Cost of Charity Care\": {'category': 'Expenses & Costs', 'subcategory': 'Uncompensated Care Costs'},\n",
    "    \"Total Bad Debt Expense\": {'category': 'Expenses & Costs', 'subcategory': 'Uncompensated Care Costs'},\n",
    "    \"Cost of Uncompensated Care\": {'category': 'Expenses & Costs', 'subcategory': 'Uncompensated Care Costs'},\n",
    "    \"Total Unreimbursed and Uncompensated Care\": {'category': 'Expenses & Costs', 'subcategory': 'Uncompensated Care Costs'},\n",
    "\n",
    "    \"Net Income\": {'category': 'Profitability', 'subcategory': None},\n",
    "    \"Net Income from Service to Patients\": {'category': 'Profitability', 'subcategory': 'Income from Patient Services'},\n",
    "\n",
    "    \"Inpatient Total Charges\": {'category': 'Charges', 'subcategory': None},\n",
    "    \"Outpatient Total Charges\": {'category': 'Charges', 'subcategory': None},\n",
    "    \"Combined Outpatient + Inpatient Total Charges\": {'category': 'Charges', 'subcategory': None},\n",
    "\n",
    "    \"Cash on Hand and in Banks\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Current Assets'},\n",
    "    \"Inventory\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Current Assets'},\n",
    "    \"Accounts Receivable\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Current Assets'},\n",
    "    \"Prepaid Expenses\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Current Assets'},\n",
    "    \"Total Current Assets\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Current Assets'},\n",
    "    \"Buildings\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Fixed Assets'},\n",
    "    \"Major Movable Equipment\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Fixed Assets'},\n",
    "    \"Total Fixed Assets\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Fixed Assets'},\n",
    "    \"Other Assets\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Other Assets'},\n",
    "    \"Total Other Assets\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Other Assets'},\n",
    "    \"Total Assets\": {'category': 'Balance Sheet - Assets', 'subcategory': 'Total Assets'},\n",
    "\n",
    "    \"Accounts Payable\": {'category': 'Balance Sheet - Liabilities', 'subcategory': 'Current Liabilities'},\n",
    "    \"Salaries, Wages, and Fees Payable\": {'category': 'Balance Sheet - Liabilities', 'subcategory': 'Current Liabilities'}, # Listed again as it's a liability\n",
    "    \"Other Current Liabilities\": {'category': 'Balance Sheet - Liabilities', 'subcategory': 'Current Liabilities'},\n",
    "    \"Total Current Liabilities\": {'category': 'Balance Sheet - Liabilities', 'subcategory': 'Current Liabilities'},\n",
    "    \"Total Long Term Liabilities\": {'category': 'Balance Sheet - Liabilities', 'subcategory': 'Long-Term Liabilities'},\n",
    "    \"Total Liabilities\": {'category': 'Balance Sheet - Liabilities', 'subcategory': 'Total Liabilities'},\n",
    "\n",
    "    \"General Fund Balance\": {'category': 'Balance Sheet - Fund Balance/Equity', 'subcategory': None},\n",
    "    \"Total Fund Balances\": {'category': 'Balance Sheet - Fund Balance/Equity', 'subcategory': None},\n",
    "    \"Total Liabilities and Fund Balances\": {'category': 'Balance Sheet - Fund Balance/Equity', 'subcategory': None},\n",
    "\n",
    "    \"Net Revenue from Medicaid\": {'category': 'Payer & Reimbursement Specific', 'subcategory': 'Medicaid Reimbursement'},\n",
    "\n",
    "    \"Cost To Charge Ratio\": {'category': 'Key Ratios', 'subcategory': None},\n",
    "}\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_and_combine_data(data_path, years):\n",
    "    \"\"\"\n",
    "    Loads and combines CSV files from specified years into a single DataFrame.\n",
    "    Adds a 'Year' column to each DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the directory containing the yearly CSV files.\n",
    "        years (range): A range of years to load data for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated DataFrame containing data from all specified years,\n",
    "                      or an empty DataFrame if no files were loaded.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    print(\"--- Step 1: Loading and Combining Data ---\")\n",
    "    for yr in years:\n",
    "        file_path = os.path.join(data_path, f\"Hospital_{yr}.csv\")\n",
    "        try:\n",
    "            # Use low_memory=False to avoid DtypeWarning with mixed types\n",
    "            df_year = pd.read_csv(file_path, low_memory=False)\n",
    "            df_year[\"Year\"] = yr\n",
    "            dfs.append(df_year)\n",
    "            print(f\"Successfully loaded data for year {yr}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {file_path}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_path}: {e}. Skipping.\")\n",
    "\n",
    "    if dfs:\n",
    "        master_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Combined data from {len(dfs)} years into a master DataFrame (initial load).\")\n",
    "        return master_df\n",
    "    else:\n",
    "        print(\"No dataframes were loaded.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame if no data loaded\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Strips leading and trailing whitespace from all column names in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    if not df.empty:\n",
    "        print(\"--- Step 2: Cleaning Column Names ---\")\n",
    "        df.columns = df.columns.str.strip()\n",
    "        print(\"Stripped whitespace from column names.\")\n",
    "    return df\n",
    "\n",
    "def clean_data_values(df):\n",
    "    \"\"\"\n",
    "    Removes specific characters (',', '.', ' ') only from the start or end\n",
    "    of string values in object columns. Preserves characters within the string.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with cleaned string values.\n",
    "    \"\"\"\n",
    "    if not df.empty:\n",
    "        print(\"--- Step 3: Cleaning Data Values ---\")\n",
    "        chars_to_strip = \",'. \" # Characters to remove from start/end\n",
    "        for col in df.columns:\n",
    "            # Apply only to columns that are of object (string) dtype\n",
    "            if df[col].dtype == 'object':\n",
    "                # Convert to string first to handle potential non-string data gracefully\n",
    "                # Apply strip method to remove leading/trailing characters\n",
    "                df[col] = df[col].astype(str).str.strip(chars_to_strip)\n",
    "        print(f\"Removed leading/trailing characters ('{chars_to_strip}') from string columns.\")\n",
    "    return df\n",
    "\n",
    "def process_fiscal_dates(df):\n",
    "    \"\"\"\n",
    "    Converts 'Fiscal Year Begin Date' and 'Fiscal Year End Date' to datetime objects\n",
    "     and extracts the year from the begin date into a new 'Fiscal Year' column.\n",
    "    Assumes date format is DD/MM/YYYY.\n",
    "    Note: This function is included for completeness but is currently skipped\n",
    "    in the main execution flow as per previous instructions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with processed fiscal date columns (if present).\n",
    "    \"\"\"\n",
    "    begin_col = 'Fiscal Year Begin Date'\n",
    "    end_col = 'Fiscal Year End Date'\n",
    "\n",
    "    if not df.empty and begin_col in df.columns and end_col in df.columns:\n",
    "        print(f\"--- Attempting to Process Fiscal Dates ('{begin_col}', '{end_col}') ---\")\n",
    "        try:\n",
    "            # Convert to datetime objects, specifying the format\n",
    "            df[begin_col] = pd.to_datetime(df[begin_col], format='%d/%m/%Y', errors='coerce')\n",
    "            df[end_col] = pd.to_datetime(df[end_col], format='%d/%m/%Y', errors='coerce')\n",
    "            print(\"Converted fiscal date columns to datetime objects.\")\n",
    "\n",
    "            # Extract the year from the begin date\n",
    "            df['Fiscal Year'] = df[begin_col].dt.year\n",
    "            print(\"Created 'Fiscal Year' column from begin date.\")\n",
    "\n",
    "            # Optional: Handle cases where conversion failed (NaNs introduced by errors='coerce')\n",
    "            if df[begin_col].isnull().any() or df[end_col].isnull().any():\n",
    "                 print(\"Warning: Some fiscal date entries could not be parsed and resulted in NaNs.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing fiscal date columns: {e}\")\n",
    "            print(\"Skipping fiscal date processing.\")\n",
    "\n",
    "    elif not df.empty:\n",
    "        missing_cols = [col for col in [begin_col, end_col] if col not in df.columns]\n",
    "        print(f\"Skipping fiscal date processing. Missing columns: {missing_cols}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_specified_columns(df, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drops a list of specified columns from the DataFrame, ignoring columns not found.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns_to_drop (list): A list of column names to drop.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with specified columns dropped.\n",
    "    \"\"\"\n",
    "    if not df.empty and columns_to_drop:\n",
    "        print(\"--- Step 4: Dropping Specified Columns ---\")\n",
    "        initial_cols = set(df.columns)\n",
    "        df = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "        dropped_cols_actual = initial_cols - set(df.columns)\n",
    "        if dropped_cols_actual:\n",
    "            print(f\"Dropped columns: {list(dropped_cols_actual)}\")\n",
    "        else:\n",
    "            present_cols_to_drop = [col for col in columns_to_drop if col in initial_cols]\n",
    "            if present_cols_to_drop:\n",
    "                 print(f\"Attempted to drop columns {present_cols_to_drop}, but they were not found after previous steps.\")\n",
    "            else:\n",
    "                 print(f\"Attempted to drop columns {columns_to_drop}, but none were found in the DataFrame.\")\n",
    "    elif not df.empty:\n",
    "        print(\"--- Step 4: No columns specified to drop ---\")\n",
    "    return df\n",
    "\n",
    "def drop_rows_missing_cbsa(df):\n",
    "    \"\"\"\n",
    "    Drops rows where 'Medicare CBSA Number' is missing or empty.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with rows containing missing CBSA dropped.\n",
    "    \"\"\"\n",
    "    if not df.empty and \"Medicare CBSA Number\" in df.columns:\n",
    "        print(\"--- Step 5: Dropping Rows with Missing CBSA ---\")\n",
    "        initial_rows = len(df)\n",
    "        # Ensure the column is treated as string for stripping and check for non-empty strings\n",
    "        df = df[\n",
    "            df[\"Medicare CBSA Number\"].notna()\n",
    "            & (df[\"Medicare CBSA Number\"].astype(str).str.strip() != \"\")\n",
    "        ]\n",
    "        rows_dropped = initial_rows - len(df)\n",
    "        print(f\"Dropped {rows_dropped} rows with missing or empty 'Medicare CBSA Number'. Remaining rows: {len(df)}\")\n",
    "    elif not df.empty:\n",
    "         print(\"--- Step 5: Skipping dropping rows with missing CBSA as 'Medicare CBSA Number' column is missing or DataFrame is empty ---\")\n",
    "    return df\n",
    "\n",
    "def create_derived_financial_columns(df):\n",
    "    \"\"\"\n",
    "    Combines staffing FTE columns into a single 'Total_Staffing_FTE' column\n",
    "    and creates a 'Total Revenue' column by summing 'Total Patient Revenue'\n",
    "    and 'Total Other Income'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new derived columns.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"--- Step 6: Skipping Derived Column Creation - DataFrame is empty ---\")\n",
    "        return df\n",
    "\n",
    "    print(\"--- Step 6: Creating Derived Financial Columns (Staffing FTE, Total Revenue) ---\")\n",
    "\n",
    "    # Combine staffing FTE\n",
    "    fte_cols = [\"FTE - Employees on Payroll\", \"Number of Interns and Residents (FTE)\"]\n",
    "    combined_fte_col_name = \"Total_Staffing_FTE\"\n",
    "    present_fte_cols = [col for col in fte_cols if col in df.columns]\n",
    "\n",
    "    if present_fte_cols:\n",
    "        print(f\"  Combining staffing FTE columns: {present_fte_cols} into '{combined_fte_col_name}'\")\n",
    "        df_fte_subset = df[present_fte_cols].copy()\n",
    "        # Convert to numeric, coercing errors to NaN, then fill NaN with 0 for summation\n",
    "        for col in df_fte_subset.columns:\n",
    "            df_fte_subset[col] = pd.to_numeric(df_fte_subset[col], errors='coerce').fillna(0)\n",
    "        df[combined_fte_col_name] = df_fte_subset.sum(axis=1)\n",
    "        print(f\"  Created '{combined_fte_col_name}' column.\")\n",
    "        # Drop original FTE columns if they exist\n",
    "        df = df.drop(columns=present_fte_cols, errors='ignore')\n",
    "        print(f\"  Dropped original staffing FTE columns: {present_fte_cols}\")\n",
    "    else:\n",
    "        print(f\"  None of the required staffing FTE columns ({fte_cols}) were found. Skipping combination.\")\n",
    "        # Create the combined column with NaNs if components are missing\n",
    "        df[combined_fte_col_name] = np.nan\n",
    "\n",
    "\n",
    "    # Create Total Revenue column\n",
    "    revenue_cols = [\"Total Patient Revenue\", \"Total Other Income\"]\n",
    "    total_revenue_col_name = \"Total Revenue\"\n",
    "    if all(col in df.columns for col in revenue_cols):\n",
    "        print(f\"  Creating '{total_revenue_col_name}' column.\")\n",
    "        # Convert to numeric, coercing errors to NaN, before summation\n",
    "        df[\"Total Patient Revenue\"] = pd.to_numeric(df[\"Total Patient Revenue\"], errors='coerce')\n",
    "        df[\"Total Other Income\"] = pd.to_numeric(df[\"Total Other Income\"], errors='coerce')\n",
    "        # Summing will result in NaN if either component is NaN\n",
    "        df[total_revenue_col_name] = df[\"Total Patient Revenue\"] + df[\"Total Other Income\"]\n",
    "        print(f\"  Created '{total_revenue_col_name}' column.\")\n",
    "    else:\n",
    "         print(f\"  Skipping Total Revenue creation. One or more required columns ({revenue_cols}) are missing or not numeric.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_facility_id(df):\n",
    "    \"\"\"\n",
    "    Creates a unique identifier for each facility using Hospital Name, City,\n",
    "    State Code, Provider Type, and Provider CCN.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the 'facility_id' column added.\n",
    "    \"\"\"\n",
    "    if not df.empty:\n",
    "        print(\"--- Step 7: Creating Facility ID ---\")\n",
    "        id_cols = [\"Hospital Name\", \"City\", \"State Code\", \"Provider Type\", \"Provider CCN\"]\n",
    "        if all(col in df.columns for col in id_cols):\n",
    "            # Ensure columns are string type before applying string methods and strip whitespace\n",
    "            for col in id_cols:\n",
    "                df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "            # Create the unique facility ID by concatenating lowercased and stripped components\n",
    "            df[\"facility_id\"] = (\n",
    "                df[\"Hospital Name\"].str.lower()\n",
    "                + \"_\"\n",
    "                + df[\"City\"].str.lower()\n",
    "                + \"_\"\n",
    "                + df[\"State Code\"].str.lower()\n",
    "                + \"_\"\n",
    "                + df[\"Provider Type\"].str.lower()\n",
    "                + \"_\"\n",
    "                + df[\"Provider CCN\"].str.lower() # Include Provider CCN for uniqueness\n",
    "            )\n",
    "            print(\"Created 'facility_id' column.\")\n",
    "        else:\n",
    "            missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "            print(f\"Skipping facility_id creation. One or more required columns ({missing_cols}) are missing.\")\n",
    "            # If essential ID columns are missing, facility_id cannot be created.\n",
    "            # This might impact subsequent steps that rely on facility_id.\n",
    "    else:\n",
    "        print(\"--- Step 7: Skipping Facility ID Creation - DataFrame is empty ---\")\n",
    "    return df\n",
    "\n",
    "def filter_facilities_all_years(df, required_years):\n",
    "    \"\"\"\n",
    "    Keeps only facilities that reported data in all specified required years.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with 'facility_id' and 'Year' columns.\n",
    "        required_years (range): A range of years that facilities must have data for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame containing only facilities present in all\n",
    "                      required years, or an empty DataFrame if filtering cannot be applied\n",
    "                      or results in no facilities.\n",
    "    \"\"\"\n",
    "    if not df.empty and \"facility_id\" in df.columns and \"Year\" in df.columns:\n",
    "        print(f\"--- Step 8: Filtering for Facilities in All {len(required_years)} Years ---\")\n",
    "        # Count the number of unique years for each facility\n",
    "        year_counts = df.groupby(\"facility_id\")[\"Year\"].nunique()\n",
    "        # Identify facility IDs that have data for the exact number of required years\n",
    "        valid_ids = year_counts[year_counts == len(required_years)].index\n",
    "        # Filter the DataFrame to keep only rows corresponding to valid facility IDs\n",
    "        df_filtered = df[df[\"facility_id\"].isin(valid_ids)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\"Filtered for facilities reporting in all {len(required_years)} file years. Remaining unique facilities: {df_filtered['facility_id'].nunique()}\")\n",
    "        return df_filtered\n",
    "    elif not df.empty:\n",
    "         print(\"--- Step 8: Skipping filtering for facilities in all years due to missing 'facility_id' or 'Year' column or DataFrame is empty ---\")\n",
    "    return pd.DataFrame() # Return empty DataFrame if filtering cannot be applied or input is empty\n",
    "\n",
    "def rename_columns(df, rename_map):\n",
    "    \"\"\"\n",
    "    Renames columns in the DataFrame based on a provided mapping.\n",
    "    Only attempts to rename columns that exist in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        rename_map (dict): A dictionary where keys are old column names and values\n",
    "                           are new column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with columns renamed according to the map.\n",
    "    \"\"\"\n",
    "    if not df.empty and rename_map:\n",
    "        print(\"--- Step 9: Renaming Columns ---\")\n",
    "        # Filter rename_map to only include columns actually in the DataFrame\n",
    "        valid_rename_map = {old_name: new_name for old_name, new_name in rename_map.items() if old_name in df.columns}\n",
    "        if valid_rename_map:\n",
    "            df = df.rename(columns=valid_rename_map)\n",
    "            print(f\"Renamed specified columns: {list(valid_rename_map.keys())}\")\n",
    "        else:\n",
    "            print(\"No columns found in DataFrame matching the rename map.\")\n",
    "    elif not df.empty:\n",
    "        print(\"--- Step 9: Rename map is empty. No columns renamed ---\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_process_inflation_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes simple yearly inflation data from a CSV file.\n",
    "    Expects columns 'Year' and 'Annual'. Converts 'Year' to integer and\n",
    "    'Annual' to numeric. Drops rows with missing essential data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the inflation data CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A processed DataFrame with 'Year' and 'Annual' columns,\n",
    "                      or an empty DataFrame if loading or processing fails.\n",
    "    \"\"\"\n",
    "    print(f\"--- Step 10: Loading and Processing Inflation Data from: {file_path} ---\")\n",
    "    try:\n",
    "        df_inflation = pd.read_csv(file_path, low_memory=False)\n",
    "        print(\"Successfully loaded inflation data.\")\n",
    "        # Ensure 'Year' and 'Annual' columns exist and are of correct types\n",
    "        if 'Year' in df_inflation.columns and 'Annual' in df_inflation.columns:\n",
    "            # Use Int64 for nullable integer year\n",
    "            df_inflation['Year'] = pd.to_numeric(df_inflation['Year'], errors='coerce').astype('Int64')\n",
    "            df_inflation['Annual'] = pd.to_numeric(df_inflation['Annual'], errors='coerce')\n",
    "            # Drop rows where Year or Annual are missing after conversion\n",
    "            df_inflation = df_inflation.dropna(subset=['Year', 'Annual']).copy() # Use .copy()\n",
    "            # Select only the relevant columns\n",
    "            df_inflation_processed = df_inflation[['Year', 'Annual']].copy() # Use .copy()\n",
    "            print(\"Inflation data processed (selected 'Year' and 'Annual', converted types).\")\n",
    "            return df_inflation_processed\n",
    "        else:\n",
    "            print(\"Error: Required columns ('Year' or 'Annual') not found in Inflation Data. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Inflation Data file not found at {file_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading/processing Inflation Data from {file_path}: {e}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_and_process_medicare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes the already processed Medicare enrollment data from a CSV file.\n",
    "    Expects columns 'YEAR', 'BENE_STATE_ABRVTN', 'TOT_BENES'. Aggregates data\n",
    "    to get yearly total beneficiaries per state. Renames columns for merging.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the processed Medicare enrollment CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: An aggregated and processed DataFrame with yearly total\n",
    "                      Medicare enrollment per state, or an empty DataFrame\n",
    "                      if loading or processing fails.\n",
    "    \"\"\"\n",
    "    print(f\"--- Step 11: Loading and Processing Medicare Enrollment Data from: {file_path} ---\")\n",
    "    try:\n",
    "        df_medicare = pd.read_csv(file_path, low_memory=False)\n",
    "        print(\"Successfully loaded processed Medicare enrollment data.\")\n",
    "\n",
    "        # Ensure required columns exist and are of correct types\n",
    "        required_cols = ['YEAR', 'BENE_STATE_ABRVTN', 'TOT_BENES']\n",
    "        if all(col in df_medicare.columns for col in required_cols):\n",
    "            # Convert types, coercing errors to NaN\n",
    "            df_medicare['YEAR'] = pd.to_numeric(df_medicare['YEAR'], errors='coerce').astype('Int64') # Use Int64 for nullable integer\n",
    "            df_medicare['TOT_BENES'] = pd.to_numeric(df_medicare['TOT_BENES'], errors='coerce')\n",
    "            df_medicare['BENE_STATE_ABRVTN'] = df_medicare['BENE_STATE_ABRVTN'].astype(str).str.strip()\n",
    "\n",
    "            # Drop rows with missing essential data for aggregation\n",
    "            df_medicare_agg = df_medicare.dropna(subset=required_cols).copy() # Use .copy()\n",
    "\n",
    "            if not df_medicare_agg.empty:\n",
    "                # Aggregate Medicare data to yearly total beneficiaries per state\n",
    "                print(\"Aggregating Medicare data to yearly total beneficiaries per state...\")\n",
    "                df_medicare_yearly = df_medicare_agg.groupby(['YEAR', 'BENE_STATE_ABRVTN'])['TOT_BENES'].sum().reset_index()\n",
    "\n",
    "                # Rename columns to match hospital data for merging\n",
    "                df_medicare_yearly = df_medicare_yearly.rename(columns={\n",
    "                    'YEAR': 'Year',\n",
    "                    'BENE_STATE_ABRVTN': 'State Code', # Assuming State Code in hospital data is abbreviation\n",
    "                    'TOT_BENES': 'Total_Yearly_Medicare_Enrollment' # New column name\n",
    "                })\n",
    "                print(\"Medicare data aggregated and columns renamed.\")\n",
    "                return df_medicare_yearly\n",
    "            else:\n",
    "                print(\"Processed Medicare data is empty after dropping NaNs for aggregation. Returning empty DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "        else:\n",
    "            missing = [col for col in required_cols if col not in df_medicare.columns]\n",
    "            print(f\"Error: Required columns for Medicare processing ({missing}) not found. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed Medicare Enrollment file not found at {file_path}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading/processing Medicare Enrollment Data from {file_path}: {e}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def impute_missing_financial_data(df):\n",
    "    \"\"\"\n",
    "    Imputes missing values in numeric columns based on the specified strategy:\n",
    "    1. Median of remaining years for the facility (if 1 or 2 missing years out of 5).\n",
    "    2. Median of the county for that year (if > 2 missing years or facility median not available).\n",
    "    3. Median of the state for that year (if county median not available).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with numeric columns to impute.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with missing numeric values imputed.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"--- Step 15: Skipping Imputation - DataFrame is empty ---\")\n",
    "        return df\n",
    "\n",
    "    print(\"\\n--- Step 15: Starting Missing Value Imputation ---\")\n",
    "\n",
    "    # Identify numeric columns to impute (exclude identifier/grouping columns)\n",
    "    # Include columns from external data if they are numeric and need imputation\n",
    "    cols_to_impute = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Ensure identifier/grouping columns are excluded if they are numeric (Year, facility_id, etc.)\n",
    "    # Add 'State Code', 'County' to exclude_list if they are numeric in your df\n",
    "    cols_to_exclude = ['Year', 'facility_id']\n",
    "\n",
    "    cols_to_impute = [col for col in cols_to_impute if col not in cols_to_exclude]\n",
    "\n",
    "    if not cols_to_impute:\n",
    "        print(\"No numeric columns found to impute. Skipping imputation.\")\n",
    "        return df\n",
    "\n",
    "    initial_missing_count = df[cols_to_impute].isnull().sum().sum()\n",
    "    print(f\"Initial total missing values in numeric columns to impute: {initial_missing_count}\")\n",
    "\n",
    "\n",
    "    for col in cols_to_impute:\n",
    "        # --- Strategy 1: Facility Median (if 1 or 2 missing years) ---\n",
    "        # Check if 'facility_id' exists before grouping\n",
    "        if 'facility_id' in df.columns:\n",
    "            # Calculate non-null counts per facility for the current column\n",
    "            # Using transform keeps the original DataFrame shape for easy masking\n",
    "            facility_counts = df.groupby('facility_id')[col].transform('count')\n",
    "\n",
    "            # Identify rows where the value is missing AND the facility has 3 or 4 non-null values (1 or 2 missing out of 5 years)\n",
    "            mask_facility_median = df[col].isnull() & (facility_counts >= 3) & (facility_counts <= 4)\n",
    "\n",
    "            if mask_facility_median.any():\n",
    "                # Calculate the median for facilities that meet the criteria\n",
    "                facility_medians = df.groupby('facility_id')[col].transform('median')\n",
    "                # Fill the missing values using the mask and the calculated facility medians\n",
    "                df.loc[mask_facility_median, col] = facility_medians[mask_facility_median]\n",
    "                # print(f\"  Filled {mask_facility_median.sum()} missing values in '{col}' with facility median.\") # Optional detailed print\n",
    "        else:\n",
    "            print(f\"Warning: 'facility_id' column not found. Skipping facility-level imputation for '{col}'.\")\n",
    "\n",
    "\n",
    "        # --- Strategy 2 & 3: County/State Median (if > 2 missing years or remaining NaNs) ---\n",
    "        # Identify remaining NaNs after attempting facility median imputation\n",
    "        mask_remaining_nans = df[col].isnull()\n",
    "\n",
    "        if mask_remaining_nans.any():\n",
    "            # print(f\"  Handling remaining NaNs in '{col}' with county/state median fallback.\") # Optional detailed print\n",
    "\n",
    "            # Calculate county median per year for the current column\n",
    "            # Ensure 'Year' in df.columns and 'County' in df.columns before grouping\n",
    "            if 'Year' in df.columns and 'County' in df.columns:\n",
    "                # Calculate median only for groups with non-null County values\n",
    "                county_medians = df.groupby(['Year', 'County'])[col].transform(lambda x: x.median() if not x.isnull().all() else np.nan)\n",
    "                # Fill the remaining NaNs using the mask and county medians\n",
    "                # This will only fill where county_medians is not NaN itself\n",
    "                df.loc[mask_remaining_nans, col] = county_medians[mask_remaining_nans]\n",
    "                # print(f\"    Attempted fill with county median. Remaining NaNs: {df[col].isnull().sum()}\") # Optional detailed print\n",
    "            else:\n",
    "                 print(f\"    Warning: 'Year' or 'County' column not found. Skipping county-level imputation for '{col}'.\")\n",
    "\n",
    "\n",
    "            # Identify any NaNs still remaining (where county median might have been NaN)\n",
    "            mask_still_remaining_nans = df[col].isnull()\n",
    "\n",
    "            if mask_still_remaining_nans.any():\n",
    "                 # Calculate state median per year for the current column\n",
    "                 # Ensure 'Year' in df.columns and 'State Code' in df.columns before grouping\n",
    "                 if 'Year' in df.columns and 'State Code' in df.columns:\n",
    "                    # Calculate median only for groups with non-null State Code values\n",
    "                    state_medians = df.groupby(['Year', 'State Code'])[col].transform(lambda x: x.median() if not x.isnull().all() else np.nan)\n",
    "                    # Fill the remaining NaNs using the mask and state medians\n",
    "                    df.loc[mask_still_remaining_nans, col] = state_medians[mask_still_remaining_nans]\n",
    "                    # print(f\"    Attempted fill with state median. Remaining NaNs: {df[col].isnull().sum()}\") # Optional detailed print\n",
    "                 else:\n",
    "                    print(f\"    Warning: 'Year' or 'State Code' column not found. Cannot use state median fallback for '{col}'.\")\n",
    "            # else: # Optional detailed print\n",
    "                 # print(f\"    No NaNs remaining after county median fill for '{col}'.\")\n",
    "\n",
    "\n",
    "    final_missing_count = df[cols_to_impute].isnull().sum().sum()\n",
    "    print(f\"Final total missing values in numeric columns after imputation: {final_missing_count}\")\n",
    "    print(\"--- Missing Value Imputation Complete ---\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_dataframe(df, output_path):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file at the specified output path.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        output_path (str): The full path (including filename) to save the CSV.\n",
    "    \"\"\"\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"\\n✅ Final Processed and Integrated DataFrame saved to: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error saving DataFrame to {output_path}: {e}\")\n",
    "    else:\n",
    "        print(\"\\n❌ DataFrame is empty. Cannot save to CSV.\")\n",
    "\n",
    "\n",
    "# --- Combined Pipeline Execution ---\n",
    "\n",
    "# Step 1: Load and combine raw data from yearly files\n",
    "df_master = load_and_combine_data(DATA_PATH, YEARS)\n",
    "\n",
    "# Check if data was loaded successfully before proceeding\n",
    "if not df_master.empty:\n",
    "\n",
    "    # Step 2: Clean column names (strip whitespace)\n",
    "    df_master = clean_column_names(df_master)\n",
    "\n",
    "    # Step 3: Clean Data Values (remove leading/trailing ',', '.', ' ' from strings)\n",
    "    df_master = clean_data_values(df_master)\n",
    "\n",
    "    # Step 4: Drop specified columns (initial drop of less relevant columns)\n",
    "    df_master = drop_specified_columns(df_master, INITIAL_COLS_TO_DROP)\n",
    "\n",
    "    # Step 5: Drop rows with missing CBSA number (essential for geographic analysis)\n",
    "    df_master = drop_rows_missing_cbsa(df_master)\n",
    "\n",
    "    # Step 6: Create Derived Financial Columns (Total Staffing FTE and Total Revenue)\n",
    "    df_master = create_derived_financial_columns(df_master)\n",
    "\n",
    "    # Step 7: Create a unique facility ID for tracking hospitals across years\n",
    "    df_master = create_facility_id(df_master)\n",
    "\n",
    "    # Step 8: Filter for facilities reporting in all specified years for consistent time series analysis\n",
    "    # All subsequent steps will operate on this smaller, filtered DataFrame.\n",
    "    df_filtered = filter_facilities_all_years(df_master, YEARS)\n",
    "\n",
    "    # Check if the filtering resulted in an empty DataFrame\n",
    "    if not df_filtered.empty:\n",
    "\n",
    "        # Step 9: Rename columns based on the RENAME_MAP\n",
    "        print(\"\\n--- Step 9: Renaming Columns (Initial and Specific) ---\")\n",
    "        df_renamed = rename_columns(df_filtered.copy(), RENAME_MAP) # Work on a copy to avoid modifying df_filtered\n",
    "\n",
    "\n",
    "        # --- Data Integration ---\n",
    "        # Step 10: Load and Process Inflation Data\n",
    "        df_inflation_processed = load_and_process_inflation_data(INFLATION_DATA_FILE)\n",
    "\n",
    "        # Step 11: Load and Process Medicare Enrollment Data\n",
    "        df_medicare_processed = load_and_process_medicare_data(PROCESSED_MEDICARE_FILE)\n",
    "\n",
    "        # Step 12: Merge Inflation Data with the main DataFrame on 'Year'\n",
    "        df_integrated = df_renamed.copy() # Start with the renamed hospital data\n",
    "        if not df_inflation_processed.empty and 'Year' in df_integrated.columns and 'Year' in df_inflation_processed.columns:\n",
    "            print(\"--- Step 12: Merging Inflation Data ---\")\n",
    "            # Ensure 'Year' is integer type for merging\n",
    "            df_integrated['Year'] = df_integrated['Year'].astype(int)\n",
    "            df_inflation_processed['Year'] = df_inflation_processed['Year'].astype(int)\n",
    "            df_integrated = pd.merge(df_integrated, df_inflation_processed, on='Year', how='left')\n",
    "            print(\"Inflation data merged.\")\n",
    "        elif not df_inflation_processed.empty:\n",
    "             print(\"--- Step 12: Skipping Inflation merge: 'Year' column missing in one or both DataFrames ---\")\n",
    "        else:\n",
    "             print(\"--- Step 12: Skipping Inflation merge: Processed Inflation Data DataFrame is empty ---\")\n",
    "\n",
    "\n",
    "        # Step 13: Merge Medicare Enrollment Data with the main DataFrame on 'Year' and 'State Code'\n",
    "        if not df_medicare_processed.empty and 'Year' in df_integrated.columns and 'State Code' in df_integrated.columns and 'Year' in df_medicare_processed.columns and 'State Code' in df_medicare_processed.columns:\n",
    "            print(\"--- Step 13: Merging Medicare Enrollment Data ---\")\n",
    "            # Ensure merge columns are of compatible types before merging\n",
    "            df_integrated['Year'] = df_integrated['Year'].astype(int)\n",
    "            df_integrated['State Code'] = df_integrated['State Code'].astype(str) # Ensure State Code is string for merging\n",
    "            df_medicare_processed['Year'] = df_medicare_processed['Year'].astype(int)\n",
    "            # State Code in medicare_processed is already string from processing\n",
    "\n",
    "            df_integrated = pd.merge(df_integrated, df_medicare_processed, on=['Year', 'State Code'], how='left')\n",
    "            print(\"Medicare Enrollment data merged.\")\n",
    "        elif not df_medicare_processed.empty:\n",
    "            print(\"--- Step 13: Skipping Medicare Enrollment merge: Required merge columns ('Year' or 'State Code') missing in one or both DataFrames ---\")\n",
    "        else:\n",
    "            print(\"--- Step 13: Skipping Medicare Enrollment merge: Processed Medicare Enrollment Data DataFrame is empty ---\")\n",
    "\n",
    "        print(\"--- Data Integration Complete ---\")\n",
    "\n",
    "\n",
    "        # Step 14: Select the final financial subset columns (including integrated data)\n",
    "        # Create the final list of columns to select, including the new ones from external data\n",
    "        final_columns_to_select = COLUMNS_FOR_FINAL_SUBSET.copy()\n",
    "        # Add columns from external data if they exist after merging\n",
    "        if 'Annual' in df_integrated.columns:\n",
    "             final_columns_to_select.append('Annual')\n",
    "        if 'Total_Yearly_Medicare_Enrollment' in df_integrated.columns:\n",
    "             final_columns_to_select.append('Total_Yearly_Medicare_Enrollment')\n",
    "\n",
    "        print(\"\\n--- Step 14: Selecting Final Financial Subset Columns ---\")\n",
    "        # Ensure all columns in final_columns_to_select are actually in df_integrated before selecting\n",
    "        # This handles cases where a column might not have been created (e.g., if external data file was missing)\n",
    "        present_final_columns = [col for col in final_columns_to_select if col in df_integrated.columns]\n",
    "        df_final_subset = df_integrated[present_final_columns].copy() # Use .copy()\n",
    "        print(f\"Selected {len(present_final_columns)} columns for the final subset.\")\n",
    "        # Report any columns requested but not found\n",
    "        missing_final_columns = [col for col in final_columns_to_select if col not in df_integrated.columns]\n",
    "        if missing_final_columns:\n",
    "             print(f\"Warning: The following columns were requested for the final subset but not found in the DataFrame: {missing_final_columns}\")\n",
    "\n",
    "\n",
    "        # Step 15: Impute missing values in the final subset using defined strategies\n",
    "        df_final_imputed = impute_missing_financial_data(df_final_subset)\n",
    "\n",
    "\n",
    "        # Step 16: Update Categorization Dictionary with new columns for documentation/analysis purposes\n",
    "        # Add entries for the new columns from external data to the global map\n",
    "        # Also ensure the renamed column is in the map if it wasn't already\n",
    "        if 'Annual' in df_final_imputed.columns and 'Annual' not in column_category_map:\n",
    "             column_category_map['Annual'] = {'category': 'External Factors', 'subcategory': 'Inflation'}\n",
    "             print(\"Added 'Annual' to categorization map.\")\n",
    "        if 'Total_Yearly_Medicare_Enrollment' in df_final_imputed.columns and 'Total_Yearly_Medicare_Enrollment' not in column_category_map:\n",
    "             column_category_map['Total_Yearly_Medicare_Enrollment'] = {'category': 'External Factors', 'subcategory': 'Medicare Enrollment'}\n",
    "             print(\"Added 'Total_Yearly_Medicare_Enrollment' to categorization map.\")\n",
    "        # Ensure the target renamed column is in the map if it wasn't already\n",
    "        if 'Hospital Total Days Or Visits' in df_final_imputed.columns and 'Hospital Total Days Or Visits' not in column_category_map:\n",
    "             # Use the category/subcategory from the original column if available, or set a default\n",
    "             original_col_name = \"Hospital Total Days (V + XVIII + XIX + Unknown) For Adults & Peds\"\n",
    "             if original_col_name in column_category_map:\n",
    "                 column_category_map['Hospital Total Days Or Visits'] = column_category_map[original_col_name]\n",
    "             else:\n",
    "                 column_category_map['Hospital Total Days Or Visits'] = {'category': 'Operations', 'subcategory': 'Capacity'} # Default\n",
    "             print(\"Ensured 'Hospital Total Days Or Visits' is in categorization map.\")\n",
    "\n",
    "\n",
    "        # --- Using the Categorization (with updated map) ---\n",
    "        # This section demonstrates how to use the categorization dictionary.\n",
    "        print(\"\\n--- Using the Categorization Dictionary with Final Data ---\")\n",
    "\n",
    "        # Example: Get all columns in the 'External Factors' category within the final subset\n",
    "        external_cols_in_subset = [col for col in df_final_imputed.columns if col in column_category_map and column_category_map[col]['category'] == 'External Factors']\n",
    "        print(f\"Columns in the 'External Factors' category within the final subset: {external_cols_in_subset}\")\n",
    "\n",
    "        # Example: Iterate through main categories and list columns present in the final subset\n",
    "        print(\"\\nColumns in Final Subset by Category (using updated map):\")\n",
    "        # Get unique categories from the updated map\n",
    "        categories = set(info['category'] for info in column_category_map.values())\n",
    "        for category in sorted(list(categories)):\n",
    "            # Find columns in the final DataFrame that belong to this category\n",
    "            cols_in_category = [col for col in df_final_imputed.columns if col in column_category_map and column_category_map[col]['category'] == category]\n",
    "            # Further filter to only include columns actually present in the final DataFrame\n",
    "            present_cols_in_category = [col for col in cols_in_category if col in df_final_imputed.columns]\n",
    "            if present_cols_in_category:\n",
    "                print(f\"- {category}: {present_cols_in_category}\")\n",
    "\n",
    "\n",
    "        # Step 17: Save the final processed, integrated, and imputed DataFrame to a CSV file\n",
    "        save_dataframe(df_final_imputed, FINAL_OUTPUT_FILE_PATH)\n",
    "\n",
    "    else:\n",
    "        print(\"\\nFiltering resulted in an empty DataFrame. No further processing will be done.\")\n",
    "\n",
    "else:\n",
    "    print(\"Pipeline stopped because no data was loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e1435-1c36-4dcc-bce1-957d198de83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
